# An Introduction to Statistical Learning (🐍 version)

## 2. Statistical learning
### 2.1 What is statistical learning?
#### Bias-variance tradeoff
Bias can be defined as the the difference as the actual values and the predicted values. High bias usually comes from an overly simplistic model, making wrong assumptions about the data, etc.

Variance comes from the the difference between predictions and predictions as the training data changes. High variance comes from an overly complex model, with too many parameters.

#### The mean squared error (MSE, 🇧🇷 erro quadrático médio)

#### Inference vs. prediction
> For example, in a real estate setting, one may seek to relate values of homes to inputs such as crime rate, zoning, distance from a river, air quality, schools, income level of community, size of houses, and so forth. In this case one might be interested in how the individual input variables affect the prices—that is, how much extra will a house be worth if it has a view of the river? This is a inference problem. Alternatively, one may simply be interested in predicting the value of a home given its characteristics: is this house under- or over-valued? This is a prediction problem.

##### Parametric vs. non-parametric methods (statistics)

#### Supervised vs. non-supervised vs. semi-supervised learning
> We tend to select statistical learning methods on the basis of whether the response is quantitative or qualitative; i.e. we might use linear regression when quantitative and logistic regression when qualitative. However, whether the predictors are qualitative or quantitative is generally considered less important. Most of the statistical learning methods discussed in this book can be applied regardless of the predictor variable type, provided that any qualitative predictors are properly coded before the analysis is performed.

### 2.4 Exercises
1. a. A flexible method would be better (higher sample size, less likely to overfit)
   b. An inflexible method would suit this case better, due to the small sample size (using a flexible method could cause overfitting)
   c. A flexible method (non-linear data)
   d. I don't know.

7. a. 
| observation |    |   |   |       |   distance  |
|-------------|----|---|---|-------|-------------|
| 1           | 0  | 3 | 0 | Red   | 3           |
| 2           | 2  | 0 | 0 | Red   | 2           |
| 3           | 0  | 1 | 3 | Red   | √10 (~3.16) |
| 4           | 0  | 1 | 2 | Green | √5 (~2.24)  |
| 5           | -1 | 0 | 1 | Green | √2 (~1.41)  |
| 6           | 1  | 1 | 1 | Red   | √3 (~1.73)  |
   b. With K = 1, the prediction would be green (considering the 5th observation)
   c. With K = 3, the prediction would be red (considering the 2nd, 5th and 6th observations)
   d. I think that if we have a small value for K, the boundary could be more flexible. So for a highly non-linear decision boundary, the best value for K should be expected to be small.